---
title: "ClimbeR Examples"
author: "John Karlen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{climbeR_examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Intoduction

Feature strength in Random Forests is difficult to assess. In their paper, "High-dimensional variable selection for survival data", Ishwaran, Hemant, et al. describe a useful metric; the minimal depth at which a variable is used to split, on average, in the forest (1). The climbeR package is intended to calculate and visualize this metric, given the result of a Random Forest experiment.

To demonstrate climbeR, first we need to run an experiment. We'll be using Ranger's Random Forest algorithm on the Iris and Veteran data sets, which will produce forests that we can climb (The the verb climb and the title of this package are an analogy for the tree traversal necessary to calculate minimal depth of a maximal subtree).

# Iris Classification

To illustrate a simple application of the climbeR package we'll be using the Iris data set, and the `ranger` R package for classification.

## Make a Random Forest

To start, lets make our forest. The following code is taken from the examples section of the Ranger documentation.

```{r, fig.show='hold'}
require(ranger)

## Classification forest with default settings
ranger(Species ~ ., data = iris)

## Prediction
train.idx <- sample(nrow(iris), 2/3 * nrow(iris))
iris.train <- iris[train.idx, ]
iris.test <- iris[-train.idx, ]
rg.iris <- ranger(Species ~ ., data = iris.train, write.forest = TRUE, importance = "impurity")
pred.iris <- predict(rg.iris, dat = iris.test)
table(iris.test$Species, pred.iris$predictions)
```

Notice that ranger has a built-in feature strength functionality - set with the `importance` argument - which can be turned on while running the experiment. Climber will offer an alternative to VIMP Variable Importance, which can be run after the experiment is finished. All climber needs is a saved forest object created by setting `write.forest = TRUE`. This output of `ranger` has the property `rg.iris$forest`. This is a snapshot of the random forest created by Ranger; it's the critical input to climbeR, which lets it calculate minimal depth of maximal subtrees.

## Using climbeR

Now that we have the result of a Random Forest experiment we can feed it to climbeR. The main function in climbeR is `getAndPlotFirstAndSecondOrderMetric`, which stands for "Get and Plot Second Order vs First Order" of the metric (minimal depth of a maximal subtree). "First Order" is the metric as described above, while "Second Order" is the second most minimal depth of a maximal subtree. When a feature is used to split at the minimal depth of a maximal subtree, if the subsetted populations contain predictive information, they may be selected to create another split and subtree, the highest of which we will call the "Second Order Minimal Depth of a Maximal Subtree". Strong features with large amounts of information will have high first and second order values for the metric. 

```{r, include=FALSE}
library(climbeR)

# call to climber function
result <- getAndPlotFirstAndSecondOrderMetric(rg.iris)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second order vs first order plot
so_vs_fo <- result$plot
```
```{r, eval=FALSE}
library(climbeR)
# call to climber function
result <- getAndPlotFirstAndSecondOrderMetric(rg.iris)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second order vs first order plot
so_vs_fo <- result$plot
```
Lets take at the `$plot` property of the `result` object, created above.
```{r, fig.width=6, fig.height=6}
# let's take a look
plot(so_vs_fo)
```

In the plot, above, we show first and second order, minimal depth of a maximal subtree, averaged across the forest. The metric's high scoring features will appear closest to the bottom left corner of the plot.

How does our feature strength metric compare to the VIMP results of ranger?

```{r}
knitr::kable(rg.iris$variable.importance)
```

In our plot, dot size represents frequency with which the feature was used to split in the forest. A boolean variable can only be used to split once because it is binary, so its split frequency will be low (1 split). A variable with high cardinality can be used to split many times, which can have the affect of saturating a tree's decision nodes. This inflates the average minimal depth of a maximal subtree for variables with high cardinality, and makes for an unfair comparison among variables of disparate cardinalities.

We can also look at the raw data:

```{r, results='asis'}
# call to climber function
result <- getAndPlotFirstAndSecondOrderMetric(rg.iris)
# evaluated data 
eval_data <- result$subtree_metrics
# another look at the result 
knitr::kable(eval_data)
```


# Random Survival Forest Feature Strength

## Veteran Survival Data Set
For a Random Survival Forest example, we'll be using the "veteran" survival data.

To start, lets make our forest
```{r, fig.show='hold'}
library(ranger)
require(survival)

# execute RSF (ranger), on the veteran data set
rg.veteran <- ranger(Surv(time, status) ~ .,
                     data = veteran,
                     write.forest = TRUE)

# retrieve result of variable importance (VIMP) for comparison later
vimp <- data.frame(rg.veteran$variable.importance)
```

The output of the call to `ranger` has a property `rg.veteran$forest`. This is a snapshot of the random survival forest created by Ranger, and climbeR can also evaluate these kinds of forests.

## Using climbeR

Now that we have the result of a Random Survival Forest experiment, we can feed it to climbeR. 

```{r, include=FALSE}
library(climbeR)
# call to climber function
result <- getAndPlotFirstAndSecondOrderMetric(rg.veteran)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```
```{r, eval=FALSE}
library(climbeR)
# call to climber function
result <- getAndPlotFirstAndSecondOrderMetric(rg.veteran)
# evaluated data 
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```
Lets take at the `$plot` property of the `result`, created above.
```{r, fig.width=6, fig.height=6}
# let's take a look
plot(so_vs_fo)
```

It is important to note that although a variable has a large value for average minimal depth (occurs in the top right corner), it may still be an important variable for the Random Forest. The subtree depth metric is biased towards continuous variables. For example, in the plot above, celltype probably has a larger first and second order depth, than karno and age. Karno and age are indeed predictive covariates, but they have high cardinality - so the metric will be biased towards them. It turns out that celltype has significant predictive information, but because it is a categorical variable with low cardinality, it is not scored highly by this metric.

As discussed in the Iris example, in the figure, dot size represents frequency with which the variable was used to split in the forest. A boolean variable can only be used to split once because it is binary, so it's dot size will be small. A variable with high cardinality can be used to split many times. Karno and age, which have high cardinality, have larger points than celltype, prior or trt variables, which are categorical covariates with low cardinality. The metric is biased towards these high cardinality features with larger point sizes. 

We can also look at the results data in table form:

```{r, results='asis'}
# another look at the result 
knitr::kable(eval_data)
```

# Evaluating with Noise Features

For more insight on the metric's bias towards continuous variables, we can add continuous noise to our dataset, and see if the metric helps us distinguish between noise and real features.

## Noise Generation

First, lets make some noise. We'll define a simple function for adding n continuous, or discrete, noise covariates to a dataset:
```{r, include = FALSE}
# We'll use some dplyr goodness here to mutate the df
library(dplyr)
```

```{r}
# this function generates noise features, which contain no information
addNoise <- function(input_df, n, discrete = FALSE){
    df.len <- length(input_df[[1]])
    for(i in 1:n){
        #add a categorical noise variable w/ cardinality: 4
        noise_vect <- rnorm(df.len)
        if(discrete){
            #create RV w/ cardinality 4 (same as celltype)
            noise_vect <- sample(c(0, 1, 2, 3), df.len, replace = TRUE)
        }
        name <- paste0("n_", toString(i))
        input_df <- input_df %>% mutate(noise_vect)
        names(input_df)[length(input_df)] <- name
    }
    return(input_df)
}
```

## Evaluate Metric with Continuous Noise
Now we're ready to add some noise, and rerun the experiment

```{r, include=FALSE}
# make some noise!
veteran <- addNoise(veteran, 5)

# rerun RSF (ranger), on the veteran data set
rg.veteran <- ranger(Surv(time, status) ~ .,
                     data = veteran,
                     write.forest = TRUE)

# call to climber function
result <- getAndPlotFirstAndSecondOrderMetric(rg.veteran)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```
```{r, eval=FALSE}
# make some noise!
veteran <- addNoise(veteran, 5)

# rerun RSF (ranger), on the veteran data set
rg.veteran <- ranger(Surv(time, status) ~ .,
                     data = veteran,
                     write.forest = TRUE)

# call to climber function
result <- getAndPlotFirstAndSecondOrderMetric(rg.veteran)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```
Lets take at the plot:
```{r, fig.width=6, fig.height=6}
# let's take a look
plot(so_vs_fo)
```

The noise variables are labeled "n_#" 1 through 5. How do the continuous noise variables rank, compared to the categorical variables? They probably beat celltype, which is bad because we know the noise features contain no information! This is why minimal depth of a maximal subtree is not a robust or perfect feature strength method.

## Evaluate Metric with Discrete Noise

To push the analysis a little further, we can see if the metric helps us distinguish between the predictive categorical variables, and some categorical variables generated from noise. We'll remove the continuous noise features just to keep the plot uncluttered, and then add 5 discrete noise features, each with cardinality 4.

```{r, include=FALSE}
# just removing the noise features by reloading the dataset
data(veteran)
# make some noise!
veteran <- addNoise(veteran, 5, discrete = TRUE)

# rerun RSF (ranger), on the veteran data set
rg.veteran <- ranger(Surv(time, status) ~ .,
                     data = veteran,
                     write.forest = TRUE)

# call to climber function
result <- getAndPlotFirstAndSecondOrderMetric(rg.veteran)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```
```{r, eval=FALSE}
# just removing the noise features by reloading the dataset
data(veteran)
# make some noise!
veteran <- addNoise(veteran, 5, discrete = TRUE)

# rerun RSF (ranger), on the veteran data set
rg.veteran <- ranger(Surv(time, status) ~ .,
                     data = veteran,
                     write.forest = TRUE)

# call to climber function
result <- getAndPlotFirstAndSecondOrderMetric(rg.veteran)
# ^ evaluated data ^
eval_data <- result$subtree_metrics
# second ord. vs first ord. plot
so_vs_fo <- result$plot
```

Lets take at the plot:
```{r, fig.width=6, fig.height=6}
# let's take a look
plot(so_vs_fo)
```

In your figure, celltype probably outranked all the categorical noise variables of the same cardinality. This is great!
It speaks to the power of the metric to differentiate between features of similar cardinality.

Citations:
1. Ishwaran, Hemant, et al. "High-dimensional variable selection for survival data." Journal of the American Statistical Association 105.489 (2010): 205-217.
